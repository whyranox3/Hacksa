{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whyranox3/Hacksa/blob/master/fityou0616.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9-28LHKY1wp"
      },
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "from IPython.display import Image as IPyImage, display\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
        "height = int(input(\"í‚¤(cm)ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \"))\n",
        "weight = int(input(\"ëª¸ë¬´ê²Œ(kg)ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \"))"
      ],
      "metadata": {
        "id": "2Q6pOW-gZFcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‚¤ êµ¬ê°„\n",
        "if 150 <= height < 160:\n",
        "    height_code = 'A'\n",
        "elif 160 <= height < 170:\n",
        "    height_code = 'B'\n",
        "elif 170 <= height < 180:\n",
        "    height_code = 'C'\n",
        "else:\n",
        "    height_code = 'C'  # ì˜ˆì™¸ ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "FeJyrbVZZFgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BMI ê³„ì‚°\n",
        "height_m = height / 100\n",
        "bmi = weight / (height_m ** 2)"
      ],
      "metadata": {
        "id": "QfOnDmsWZFjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì²´ì¤‘ êµ¬ê°„\n",
        "if bmi < 18.5:\n",
        "    weight_code = 'under'\n",
        "elif bmi < 25:\n",
        "    weight_code = 'normal'\n",
        "else:\n",
        "    weight_code = 'over'"
      ],
      "metadata": {
        "id": "ygzWtAayZFlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì•„ë°”íƒ€ ì´ë¯¸ì§€ ì„ íƒ\n",
        "filename = f\"{height_code}_{weight_code}.jpg\"\n",
        "avatar_path = f\"/content/{filename}\"\n",
        "print(f\"ì²´í˜•: {filename}\")\n",
        "\n",
        "avatar = Image.open(avatar_path).convert(\"RGBA\")\n",
        "display(avatar)"
      ],
      "metadata": {
        "id": "49WUAsSTZFnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position_table = {\n",
        "    \"A_under\": {\n",
        "        \"upper\": {\"center\": (120, 183), \"size\": (160, 170)},\n",
        "        \"lower\": {\"center\": (120, 305), \"size\": (150, 170)},\n",
        "    },\n",
        "    \"A_normal\": {\n",
        "        \"upper\": {\"center\": (120, 183), \"size\": (165, 175)},\n",
        "        \"lower\": {\"center\": (120, 305), \"size\": (155, 175)},\n",
        "    },\n",
        "    \"A_over\": {\n",
        "        \"upper\": {\"center\": (110, 140), \"size\": (170, 180)},\n",
        "        \"lower\": {\"center\": (110, 320), \"size\": (160, 180)},\n",
        "    },\n",
        "    \"B_under\": {\n",
        "        \"upper\": {\"center\": (120, 183), \"size\": (170, 180)},\n",
        "        \"lower\": {\"center\": (120, 305), \"size\": (160, 180)},\n",
        "    },\n",
        "    \"B_normal\": {\n",
        "        \"upper\": {\"center\": (120, 145), \"size\": (175, 185)},\n",
        "        \"lower\": {\"center\": (120, 330), \"size\": (165, 185)},\n",
        "    },\n",
        "    \"B_over\": {\n",
        "        \"upper\": {\"center\": (120, 150), \"size\": (180, 190)},\n",
        "        \"lower\": {\"center\": (120, 340), \"size\": (170, 190)},\n",
        "    },\n",
        "    \"C_under\": {\n",
        "        \"upper\": {\"center\": (130, 150), \"size\": (180, 190)},\n",
        "        \"lower\": {\"center\": (130, 350), \"size\": (170, 190)},\n",
        "    },\n",
        "    \"C_normal\": {\n",
        "        \"upper\": {\"center\": (130, 170), \"size\": (185, 195)},\n",
        "        \"lower\": {\"center\": (130, 350), \"size\": (175, 195)},\n",
        "    },\n",
        "    \"C_over\": {\n",
        "        \"upper\": {\"center\": (130, 160), \"size\": (190, 200)},\n",
        "        \"lower\": {\"center\": (130, 370), \"size\": (180, 200)},\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "XBkILdgekH_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì²´í˜• ì½”ë“œ ì¶”ì¶œ\n",
        "code = filename.split('.')[0]  # ì˜ˆ: 'B_normal'"
      ],
      "metadata": {
        "id": "sdXeTOrlkHwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìœ„ì¹˜/í¬ê¸° í…Œì´ë¸”ì—ì„œ í•´ë‹¹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "upper_info = position_table[code][\"upper\"]\n",
        "lower_info = position_table[code][\"lower\"]"
      ],
      "metadata": {
        "id": "jqLN47fokcbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# U-2-Net ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„\n",
        "!git clone https://github.com/xuebinqin/U-2-Net\n",
        "%cd U-2-Net\n",
        "!pip install gdown opencv-python pillow numpy\n",
        "\n",
        "%cd saved_models\n",
        "!gdown --id 1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy\n",
        "%cd .."
      ],
      "metadata": {
        "id": "-Wjrqc2_ZR-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜· ì €ì¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„\n",
        "upper_dir = \"outputs/upper_clothes\"\n",
        "lower_dir = \"outputs/lower_clothes\"\n",
        "os.makedirs(upper_dir, exist_ok=True)\n",
        "os.makedirs(lower_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "MolAnk2jZFp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒì˜ ì—…ë¡œë“œ\n",
        "print(\"ìƒì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\")\n",
        "uploaded_upper = files.upload()\n",
        "upper_filename = \"shirt.jpg\"\n",
        "\n",
        "# ì—…ë¡œë“œëœ ìƒì˜ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ\n",
        "upper_input_path = None # ë‚˜ì¤‘ì— ëˆ„ë¼ ì²˜ë¦¬ì— ì‚¬ìš©\n",
        "for upper_filename in uploaded_upper:\n",
        "    upper_input_path = os.path.join(upper_dir, upper_filename)\n",
        "    Image.open(upper_filename).save(upper_input_path)\n",
        "    print(f\"ìƒì˜ ì €ì¥ ì™„ë£Œ: {upper_input_path}\")"
      ],
      "metadata": {
        "id": "Ljkm9NeeZXup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•˜ì˜ ì—…ë¡œë“œ\n",
        "print(\"í•˜ì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\")\n",
        "uploaded_lower = files.upload()\n",
        "lower_filename = \"pants.jpg\"\n",
        "\n",
        "for fname in uploaded_lower:\n",
        "    lower_input_path = os.path.join(lower_dir, fname)\n",
        "    Image.open(fname).save(lower_input_path)\n",
        "    print(f\"í•˜ì˜ ì €ì¥ ì™„ë£Œ: {lower_input_path}\")"
      ],
      "metadata": {
        "id": "lTBhbBEXZFsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# U-2-Net ê´€ë ¨ ëª¨ë“ˆ\n",
        "from model import U2NETP\n",
        "from u2net_test import normPRED, save_output\n",
        "from data_loader import RescaleT, ToTensorLab, ToTensor\n",
        "\n",
        "# U-2-Net ëˆ„ë¼ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def process_with_u2net(input_path, output_dir, model_name='u2net'):\n",
        "  print(f\"[U-2-Net ì²˜ë¦¬ ì‹œì‘] {input_path}\")"
      ],
      "metadata": {
        "id": "PAB9XCIOZFu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë¡œë”©\n",
        "model_dir = 'saved_models/u2netp.pth'\n",
        "net = U2NETP(3, 1)\n",
        "net.load_state_dict(torch.load(model_dir, map_location='cpu'))\n",
        "net.eval()"
      ],
      "metadata": {
        "id": "CYvCh7FYZF1C",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from data_loader import RescaleT, ToTensorLab\n",
        "from u2net_test import normPRED, save_output\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "b8mLnJgQfgOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ transform ì •ì˜\n",
        "transform = transforms.Compose([\n",
        "    RescaleT(320),\n",
        "    ToTensorLab(flag=0)\n",
        "])"
      ],
      "metadata": {
        "id": "0wCOkIDhfgRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_output(image_path, pred, d_dir, file_name=None):\n",
        "    pred = pred.squeeze()\n",
        "    pred = pred.cpu().data.numpy()\n",
        "    mask = Image.fromarray((pred * 255).astype(np.uint8)).convert('L')\n",
        "\n",
        "    # ì´ë¯¸ì§€ í¬ê¸° ë§ì¶”ê¸° (í•µì‹¬!)\n",
        "    original = Image.open(image_path).convert(\"RGBA\")\n",
        "    mask = mask.resize(original.size)  # ğŸ’¡ ì—¬ê¸°ê°€ í•µì‹¬\n",
        "\n",
        "    if file_name is None:\n",
        "        file_name = os.path.basename(image_path)\n",
        "    full_path = os.path.join(d_dir, file_name)\n",
        "\n",
        "    empty = Image.new(\"RGBA\", original.size, (0, 0, 0, 0))\n",
        "    result = Image.composite(original, empty, mask)\n",
        "\n",
        "    result.save(full_path)"
      ],
      "metadata": {
        "id": "p0XtFvV6hIk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëˆ„ë¼ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def inference_with_u2net(image_path, output_dir):\n",
        "    # ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    img = np.array(Image.open(image_path).convert('RGB'))\n",
        "    sample = {'image': img, 'label': img, 'imidx': np.array([0])}\n",
        "\n",
        "    transformed = transform(sample)  # ì´ê±¸ ë¨¼ì € í•´ì•¼ transformedê°€ ìƒê¹€\n",
        "\n",
        "    img_tensor = transformed['image'].unsqueeze(0).type(torch.FloatTensor)\n",
        "\n",
        "    # ì¶”ë¡ \n",
        "    with torch.no_grad():\n",
        "        d1, _, _, _, _, _, _ = net(img_tensor)\n",
        "        pred = d1[:, 0, :, :]\n",
        "        pred = normPRED(pred)\n",
        "\n",
        "    # ì €ì¥\n",
        "    filename = os.path.splitext(os.path.basename(image_path))[0]\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    save_output(image_path, pred, output_dir, filename + \"_u2net.png\")"
      ],
      "metadata": {
        "id": "fRkuzLhVfgT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ë ‰í† ë¦¬ ì•ˆ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬\n",
        "def process_all_images(input_dir, output_dir):\n",
        "    for fname in os.listdir(input_dir):\n",
        "        if fname.lower().endswith((\".jpg\", \".png\")):\n",
        "            full_path = os.path.join(input_dir, fname)\n",
        "            inference_with_u2net(full_path, output_dir)\n",
        "            print(f\"{fname} ëˆ„ë¼ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "uz4lAL8vfgWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒì˜ ë° í•˜ì˜ ì²˜ë¦¬ ì‹¤í–‰\n",
        "process_all_images(\"outputs/upper_clothes\", \"outputs/upper_clothes\")\n",
        "process_all_images(\"outputs/lower_clothes\", \"outputs/lower_clothes\")"
      ],
      "metadata": {
        "id": "-NaXHz_ZfgaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# ìƒì˜ í™•ì¸\n",
        "upper_path = \"outputs/upper_clothes\"\n",
        "for f in os.listdir(upper_path):\n",
        "    if f.endswith(\"_u2net.png\"):\n",
        "        print(f\"ìƒì˜ ëˆ„ë¼ ê²°ê³¼: {f}\")\n",
        "        display(Image.open(os.path.join(upper_path, f)))\n",
        "\n",
        "# í•˜ì˜ í™•ì¸\n",
        "lower_path = \"outputs/lower_clothes\"\n",
        "for f in os.listdir(lower_path):\n",
        "    if f.endswith(\"_u2net.png\"):\n",
        "        print(f\"í•˜ì˜ ëˆ„ë¼ ê²°ê³¼: {f}\")\n",
        "        display(Image.open(os.path.join(lower_path, f)))\n",
        "\n",
        "# í•´ë‹¹ ë¶€ë¶„ê¹Œì§€ ëˆ„ë¼ ì™„ë£Œ. ëˆ„ë¼ ë¶€ë¶„ì€ ê±´ë“œë¦´ ê²ƒ ì—†ìŒ. ì•„ë°”íƒ€ ë¶„ë¦¬ ë° í•©ì„±ë§Œ"
      ],
      "metadata": {
        "collapsed": true,
        "id": "igY9PHwLhg22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def paste_centered(base, overlay, center_xy, size):\n",
        "    resized = overlay.resize(size)\n",
        "    cx, cy = center_xy\n",
        "    w, h = resized.size\n",
        "    pos = (cx - w // 2, cy - h // 2)\n",
        "    base.paste(resized, pos, resized)\n",
        "    return base"
      ],
      "metadata": {
        "id": "Xp47s4EBhg5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avatar = Image.open(avatar_path).convert(\"RGBA\")\n",
        "shirt = Image.open(\"outputs/upper_clothes/tshirt_u2net.png\").convert(\"RGBA\")\n",
        "pants = Image.open(\"outputs/lower_clothes/hpants_u2net.png\").convert(\"RGBA\")\n",
        "\n",
        "avatar = paste_centered(avatar, shirt, upper_info[\"center\"], upper_info[\"size\"])\n",
        "avatar = paste_centered(avatar, pants, lower_info[\"center\"], lower_info[\"size\"])\n",
        "\n",
        "display(avatar)"
      ],
      "metadata": {
        "id": "gS9n-urNhg7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rit0slaQhg9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thDZJbmJhg_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "94w1dZlohhDS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}